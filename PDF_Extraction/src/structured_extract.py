"""Docling ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ë¥¼ êµ¬ì¡°í™”í•´ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸.

í˜ì´ì§€ë³„ Markdown, í‘œ JSON/ì´ë¯¸ì§€, ê·¸ë¦¼ ì´ë¯¸ì§€ë¥¼ í•œ í´ë”ì— ëª¨ìœ¼ê³ 
í›„ì† GPT í•´ì„ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë©”íƒ€ë°ì´í„°(JSON)ë¥¼ ì‘ì„±í•œë‹¤.
"""

from __future__ import annotations

import argparse
import base64
import json
import os
from dataclasses import dataclass
from io import BytesIO
from pathlib import Path
from typing import Iterable, List

import pypdfium2 as pdfium
from docling.datamodel.base_models import ConversionStatus
from docling.document_converter import DocumentConverter
from dotenv import load_dotenv
from openai import OpenAI
from collections import Counter
import re
import html


REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_INPUT_DIR = REPO_ROOT / "data" / "input"
DEFAULT_OUTPUT_DIR = REPO_ROOT / "data" / "pages_structured"
GPT_API_KEY_PLACEHOLDER = "PASTE_YOUR_GPT_API_KEY"
MIN_FIGURE_AREA_RATIO = 0.01
FIGURE_HEADER_RATIO = 0.12

load_dotenv()


@dataclass
class TextBlock:
    text: str
    bbox: dict[str, float]


def normalize_line(text: str) -> str:
    """Token Reduction Strategy 2 Normalization"""
    # 1. HTML Unescape
    text = html.unescape(text)
    # 2. ìˆ«ì ë§ˆìŠ¤í‚¹
    text = re.sub(r'\d+', 'N', text)
    # 3. ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ë‹¨ìˆœí™”
    text = re.sub(r'\s+', ' ', text)
    # 4. ì†Œë¬¸ìí™” ë° ì–‘ì˜† ê³µë°± ì œê±°
    return text.strip().lower()


def analyze_batch_patterns(doc, target_pages: list[int]) -> set[str]:
    """
    ë°°ì¹˜ ë‚´ ê³µí†µ í—¤ë”/í‘¸í„° íŒ¨í„´ ì‹ë³„.
    - ì •ê·œí™”ëœ ë¼ì¸ì˜ 'ì•ë¶€ë¶„(Prefix)'ì„ ê¸°ì¤€ìœ¼ë¡œ ë¹ˆë„ ë¶„ì„.
    - ê¸°ì¤€: ë°°ì¹˜ ë‚´ 20% ì´ìƒ í˜ì´ì§€ ë“±ì¥ (ìµœì†Œ 3í˜ì´ì§€).
    """
    prefix_counts = Counter()
    page_count = len(target_pages)
    
    if page_count < 3:
        return set()

    # Prefix ê¸¸ì´ ì„¤ì • (ê°€ë³€ì ì¸ ë’·ë¶€ë¶„ ë¬´ì‹œ)
    PREFIX_LEN = 15

    for page_no in target_pages:
        try:
            md = doc.export_to_markdown(page_no=page_no, include_annotations=False)
        except ValueError:
            continue
            
        seen_in_page = set()
        for line in md.split('\n'):
            line = line.strip()
            if not line: continue
            if '[image]' in line.lower(): continue
            
            norm = normalize_line(line)
            if len(norm) < 4: continue
            
            # Use Prefix as key
            key = norm[:PREFIX_LEN]
            
            if key not in seen_in_page:
                prefix_counts[key] += 1
                seen_in_page.add(key)

    # Threshold: 20% or 3 pages
    threshold = max(3, int(page_count * 0.2))
    
    # Return set of "Bad Prefixes"
    common_prefixes = {p for p, count in prefix_counts.items() if count >= threshold}
    
    if common_prefixes:
        print(f"INFO: Detected {len(common_prefixes)} common pattern categories (Threshold: {threshold}/{page_count})")
        
    return common_prefixes


def infer_default_pdf() -> Path:
    candidates = sorted(DEFAULT_INPUT_DIR.glob("*.pdf"))
    if not candidates:
        raise FileNotFoundError(
            f"{DEFAULT_INPUT_DIR}ì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. --pdf ì˜µì…˜ìœ¼ë¡œ ì§ì ‘ ì§€ì •í•˜ì„¸ìš”."
        )
    return candidates[0]


def parse_page_selection(selection: str, total_pages: int) -> list[int]:
    pages: set[int] = set()
    for raw in selection.split(","):
        part = raw.strip()
        if not part:
            continue
        if "-" in part:
            start_str, end_str = part.split("-", 1)
            start = int(start_str)
            end = int(end_str)
            if start > end:
                start, end = end, start
            pages.update(range(start, end + 1))
        else:
            pages.add(int(part))

    normalized = sorted(p for p in pages if 1 <= p <= total_pages)
    if not normalized:
        raise ValueError("ìš”ì²­í•œ í˜ì´ì§€ê°€ ë¬¸ì„œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.")
    return normalized


def pages_from_count(total_pages: int, count: int) -> list[int]:
    if count <= 0:
        raise ValueError("--count ê°’ì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    upper = min(total_pages, count)
    return list(range(1, upper + 1))


def chunk_consecutive(pages: Iterable[int]) -> list[tuple[int, int]]:
    sorted_pages = sorted(pages)
    if not sorted_pages:
        return []

    groups: list[tuple[int, int]] = []
    start = prev = sorted_pages[0]
    for page in sorted_pages[1:]:
        if page == prev + 1:
            prev = page
            continue
        groups.append((start, prev))
        start = prev = page
    groups.append((start, prev))
    return groups


def bbox_to_dict(bbox) -> dict[str, float]:
    return {
        "left": float(bbox.l),
        "right": float(bbox.r),
        "top": float(bbox.t),
        "bottom": float(bbox.b),
    }


def collect_text_blocks(doc, page_no: int) -> list[TextBlock]:
    blocks: list[TextBlock] = []
    for text in doc.texts:
        for prov in text.prov:
            if prov.page_no != page_no or prov.bbox is None:
                continue
            content = (text.text or "").strip()
            if not content:
                continue
            blocks.append(TextBlock(text=content, bbox=bbox_to_dict(prov.bbox)))
            break
    return blocks


def horizontal_overlap_ratio(a: dict[str, float], b: dict[str, float]) -> float:
    left = max(a["left"], b["left"])
    right = min(a["right"], b["right"])
    if right <= left:
        return 0.0
    width_a = max(1e-3, a["right"] - a["left"])
    width_b = max(1e-3, b["right"] - b["left"])
    overlap = right - left
    return overlap / min(width_a, width_b)


def detect_table_title(
    table_bbox: dict[str, float],
    text_blocks: list[TextBlock],
    vertical_threshold: float = 120.0,
    overlap_threshold: float = 0.5,
    max_chars: int = 60,
) -> str | None:
    best_text: str | None = None
    best_score = float("inf")
    for block in text_blocks:
        overlap = horizontal_overlap_ratio(block.bbox, table_bbox)
        if overlap < overlap_threshold:
            continue
        text_len = len(block.text)
        if text_len == 0 or text_len > max_chars:
            continue
        vertical_gap = block.bbox["bottom"] - table_bbox["top"]
        if vertical_gap < 0 or vertical_gap > vertical_threshold:
            continue
        score = vertical_gap - overlap * 10
        if score < best_score:
            best_score = score
            best_text = block.text
    return best_text


def table_cells_to_json(table, doc) -> list[list[dict[str, object]]]:
    rows: list[list[dict[str, object]]] = []
    for row_idx, row in enumerate(table.data.grid):
        row_cells: list[dict[str, object]] = []
        for col_idx, cell in enumerate(row):
            row_cells.append(
                {
                    "row": row_idx,
                    "col": col_idx,
                    "text": cell.text if cell.text is not None else cell._get_text(doc=doc),
                    "row_span": cell.row_span,
                    "col_span": cell.col_span,
                    "row_header": cell.row_header,
                    "column_header": cell.column_header,
                }
            )
        rows.append(row_cells)
    return rows


def render_page_image(pdf_doc: pdfium.PdfDocument, page_no: int, scale: float):
    page = pdf_doc[page_no - 1]
    try:
        pil_image = page.render(scale=scale).to_pil()
    finally:
        page.close()
    return pil_image


def get_pdf_page_size(pdf_doc: pdfium.PdfDocument, page_no: int) -> tuple[float, float]:
    page = pdf_doc[page_no - 1]
    try:
        width, height = page.get_size()
    finally:
        page.close()
    return float(width), float(height)


def image_to_data_url(image) -> str:
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    encoded = base64.b64encode(buffer.getvalue()).decode("ascii")
    return f"data:image/png;base64,{encoded}"


def extract_response_text(response) -> str:
    texts: list[str] = []
    for item in getattr(response, "output", []) or []:
        for content in getattr(item, "content", []) or []:
            if getattr(content, "type", None) == "output_text":
                texts.append(getattr(content, "text", ""))
    return "".join(texts).strip()


def bbox_to_pixels(
    bbox: dict[str, float],
    page_width: float,
    page_height: float,
    image_width: int,
    image_height: int,
):
    scale_x = image_width / page_width
    scale_y = image_height / page_height

    left = max(0, int(round(bbox["left"] * scale_x)))
    right = min(image_width, int(round(bbox["right"] * scale_x)))
    top = max(0, int(round((page_height - bbox["top"]) * scale_y)))
    bottom = min(image_height, int(round((page_height - bbox["bottom"]) * scale_y)))

    if bottom <= top or right <= left:
        return None
    return left, top, right, bottom


def crop_region(image, crop_box, output_path: Path) -> Path | None:
    if crop_box is None:
        return None
    region = image.crop(crop_box)
    region.save(output_path)
    return output_path


def summarize_with_gpt(page_payload: dict, api_key: str | None, model_name: str) -> str:
    """GPT í˜¸ì¶œ ìë¦¬. API í‚¤ë¥¼ ì„¤ì •í•œ ë’¤ ì›í•˜ëŠ” ë¡œì§ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”."""

    if not api_key or api_key == GPT_API_KEY_PLACEHOLDER:
        return "GPT ìš”ì•½ ë¯¸ì‹¤í–‰: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  summarize_with_gptë¥¼ êµ¬í˜„í•˜ì„¸ìš”."

    client = OpenAI(api_key=api_key)

    page_no = page_payload["page_number"]
    markdown = page_payload["markdown"]
    trimmed_markdown = markdown[:6000]

    table_notes = []
    for table in page_payload.get("tables", []):
        title = table.get("title") or table.get("id")
        detail = f"- {title}: êµ¬ì¡°í™” ë°ì´í„°={table.get('json_path')}"
        if table.get("ocr_path"):
            detail += f", OCR={table.get('ocr_path')}"
        table_notes.append(detail)
    tables_text = "\n".join(table_notes) if table_notes else "(ì´ í˜ì´ì§€ì—ëŠ” í‘œê°€ ì—†ìŠµë‹ˆë‹¤.)"

    visual_hint = "ì´ í˜ì´ì§€ëŠ” ì´ë¯¸ì§€ ë¹„ì¤‘ì´ ë†’ìœ¼ë‹ˆ ì£¼ìš” ë©”ì‹œì§€ë¥¼ ì„œìˆ í•˜ê³ , ìˆ˜ì¹˜ ë¹„êµëŠ” table_jsonì„ ì°¸ê³ í•˜ì„¸ìš”."
    prompt = (
        f"ë‹¤ìŒì€ ESG ë³´ê³ ì„œ {page_no}í˜ì´ì§€ì˜ Markdown ë³¸ë¬¸ì…ë‹ˆë‹¤. ìš”ì•½ê³¼ í•´ì„ì„ ì œê³µí•˜ë˜, "
        f"êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ëŠ” ì•„ë˜ table_json íŒŒì¼ì—ì„œ ê²€ì¦ í›„ ì–¸ê¸‰í•˜ì„¸ìš”.\n\n"
        f"[ë³¸ë¬¸]\n{trimmed_markdown}\n\n[í‘œ ë©”íƒ€]\n{tables_text}\n\n{visual_hint}"
    )

    try:
        response = client.responses.create(
            model=model_name,
            input=[
                {
                    "role": "system",
                    "content": [
                        {
                            "type": "text",
                            "text": "ë‹¹ì‹ ì€ ESG ë³´ê³ ì„œë¥¼ í•´ì„í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤. í‘œ JSONì˜ ìˆ«ìë¥¼ ì‹ ë¢°í•˜ê³ , Markdown í…ìŠ¤íŠ¸ì˜ ë§¥ë½ì„ ì„¤ëª…í•˜ì„¸ìš”.",
                        }
                    ],
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}],
                },
            ],
            temperature=0.2,
            max_output_tokens=600,
        )
    except Exception as exc:  # pragma: no cover - runtime safeguard
        return f"GPT í˜¸ì¶œ ì‹¤íŒ¨: {exc}"

    output_fragments: list[str] = []
    for item in getattr(response, "output", []) or []:
        for content in getattr(item, "content", []) or []:
            text_val = getattr(content, "text", None)
            if text_val:
                output_fragments.append(text_val)
    return "".join(output_fragments).strip() or "(GPT ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.)"


def process_page(
    doc,
    pdf_doc: pdfium.PdfDocument,
    page_no: int,
    output_root: Path,
    render_scale: float,
    gpt_api_key: str | None,
    enable_gpt: bool,
    gpt_model: str,
    visual_threshold: float,
    clean_patterns: set[str] = None,
):
    page_dir = output_root / f"page_{page_no:04d}"
    tables_dir = page_dir / "tables"
    figures_dir = page_dir / "figures"
    tables_dir.mkdir(parents=True, exist_ok=True)
    figures_dir.mkdir(parents=True, exist_ok=True)

    raw_markdown = doc.export_to_markdown(
        page_no=page_no,
        image_placeholder="[IMAGE]",
        include_annotations=False,
        page_break_placeholder=None,
    ).strip()

    # Apply Token Reduction (Strategy 2)
    if clean_patterns:
        cleaned_lines = []
        PREFIX_LEN = 15
        
        for line in raw_markdown.split('\n'):
            # [IMAGE] íƒœê·¸ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŒ
            if '[image]' in line.lower():
                cleaned_lines.append(line)
                continue
            
            # íŒ¨í„´ ë§¤ì¹­ í™•ì¸ (Prefix Match)
            norm = normalize_line(line)
            key = norm[:PREFIX_LEN]
            
            if key in clean_patterns:
                continue # Skip common header/footer
            
            cleaned_lines.append(line)
        markdown = "\n".join(cleaned_lines).strip()
    else:
        markdown = raw_markdown

    page_md_path = page_dir / "page.md"
    page_md_path.write_text(markdown, encoding="utf-8")

    page_image = render_page_image(pdf_doc, page_no, render_scale)
    page_image_path = page_dir / "page.png"
    page_image.save(page_image_path)

    page_size = doc.pages[page_no].size
    page_width = float(page_size.width)
    page_height = float(page_size.height)

    text_blocks = collect_text_blocks(doc, page_no)

    tables_meta: list[dict[str, object]] = []
    table_area = 0.0
    page_tables = [t for t in doc.tables if any(prov.page_no == page_no for prov in t.prov)]
    for idx, table in enumerate(page_tables, start=1):
        bbox = bbox_to_dict(table.prov[0].bbox)
        title = detect_table_title(bbox, text_blocks)
        table_id = f"table_{idx:03d}"
        markdown_path = tables_dir / f"{table_id}.md"
        markdown_path.write_text(table.export_to_markdown(doc=doc), encoding="utf-8")

        table_json = {
            "id": table_id,
            "title": title,
            "cells": table_cells_to_json(table, doc),
        }
        json_path = tables_dir / f"{table_id}.json"
        json_path.write_text(json.dumps(table_json, ensure_ascii=False, indent=2), encoding="utf-8")

        crop_box = bbox_to_pixels(bbox, page_width, page_height, page_image.width, page_image.height)
        image_path = tables_dir / f"{table_id}.png"
        saved_image = crop_region(page_image, crop_box, image_path)

        tables_meta.append(
            {
                "id": table_id,
                "title": title,
                "markdown_path": str(markdown_path.relative_to(output_root)),
                "json_path": str(json_path.relative_to(output_root)),
                "image_path": str(saved_image.relative_to(output_root)) if saved_image else "",
                "bbox": bbox,
            }
        )
        table_area += max(0.0, (bbox["right"] - bbox["left"])) * max(0.0, (bbox["top"] - bbox["bottom"]))

    figures_meta: list[dict[str, object]] = []
    figure_area = 0.0
    header_cutoff = page_height * (1 - FIGURE_HEADER_RATIO) if page_height else None
    page_pictures = [p for p in doc.pictures if any(prov.page_no == page_no for prov in p.prov)]
    for idx, picture in enumerate(page_pictures, start=1):
        bbox = bbox_to_dict(picture.prov[0].bbox)
        figure_id = f"figure_{idx:03d}"
        width = max(0.0, bbox["right"] - bbox["left"])
        height = max(0.0, bbox["top"] - bbox["bottom"])
        page_area = max(1e-3, page_width * page_height)
        area_ratio = (width * height) / page_area
        if area_ratio < MIN_FIGURE_AREA_RATIO:
            print(
                f"[SKIP ICON] page {page_no} {figure_id} (area ratio={area_ratio:.4f})"
            )
            continue
        if header_cutoff and bbox["bottom"] >= header_cutoff:
            print(f"[SKIP HEADER] page {page_no} {figure_id} (header zone)")
            continue
        crop_box = bbox_to_pixels(bbox, page_width, page_height, page_image.width, page_image.height)
        image_path = figures_dir / f"{figure_id}.png"
        saved_image = crop_region(page_image, crop_box, image_path)

        caption_texts: list[str] = []
        for ref in picture.captions:
            node = ref.resolve(doc)
            text = getattr(node, "text", "")
            if text:
                caption_texts.append(text.strip())

        figures_meta.append(
            {
                "id": figure_id,
                "caption": " ".join(caption_texts) if caption_texts else None,
                "image_path": str(saved_image.relative_to(output_root)) if saved_image else "",
                "bbox": bbox,
            }
        )
        figure_area += width * height

    visual_density = (table_area + figure_area) / (page_width * page_height)
    needs_visual_review = visual_density >= visual_threshold or bool(page_pictures)

    summary_path = None
    if enable_gpt:
        summary_text = summarize_with_gpt(
            {
                "page_number": page_no,
                "markdown": markdown,
                "tables": tables_meta,
            },
            gpt_api_key,
            gpt_model,
        )
        summary_path = page_dir / "summary.md"
        summary_path.write_text(summary_text, encoding="utf-8")

    page_payload = {
        "page_number": page_no,
        "markdown": markdown,
        "markdown_path": str(page_md_path.relative_to(output_root)),
        "page_image_path": str(page_image_path.relative_to(output_root)),
        "page_dimensions": {"width": page_width, "height": page_height},
        "tables": tables_meta,
        "figures": figures_meta,
        "needs_visual_review": needs_visual_review,
        "visual_density": visual_density,
        "summary_path": str(summary_path.relative_to(output_root)) if summary_path else None,
    }

    page_json_path = page_dir / "page.json"
    page_json_path.write_text(json.dumps(page_payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"í˜ì´ì§€ {page_no} ì²˜ë¦¬ ì™„ë£Œ -> {page_json_path}")


def build_markdown_from_gpt(data: dict) -> str:
    parts: list[str] = []
    summary = (data.get("summary") or "").strip()
    if summary:
        parts.append(summary)

    key_points = data.get("key_points") or []
    if key_points:
        bullets = "\n".join(f"- {point}" for point in key_points if point)
        if bullets:
            parts.append(bullets)

    narrative = (data.get("notes") or "").strip()
    if narrative:
        parts.append(narrative)
    return "\n\n".join(parts).strip()


def write_gpt_tables(
    tables: list[dict],
    tables_dir: Path,
    output_root: Path,
) -> list[dict]:
    meta: list[dict] = []
    for idx, table in enumerate(tables or [], start=1):
        title = table.get("title") or f"GPT Table {idx}"
        headers = table.get("headers") or []
        rows = table.get("rows") or []
        table_id = f"table_{idx:03d}"

        table_md_lines: list[str] = []
        if headers:
            table_md_lines.append("| " + " | ".join(str(h) for h in headers) + " |")
            table_md_lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
        for row in rows:
            table_md_lines.append("| " + " | ".join(str(cell) for cell in row) + " |")
        markdown_content = "\n".join(table_md_lines) if table_md_lines else title

        md_path = tables_dir / f"{table_id}.md"
        md_path.write_text(markdown_content, encoding="utf-8")

        table_json = {
            "id": table_id,
            "title": title,
            "headers": headers,
            "rows": rows,
            "source": "gpt_fallback",
        }
        json_path = tables_dir / f"{table_id}.json"
        json_path.write_text(json.dumps(table_json, ensure_ascii=False, indent=2), encoding="utf-8")

        meta.append(
            {
                "id": table_id,
                "title": title,
                "markdown_path": str(md_path.relative_to(output_root)),
                "json_path": str(json_path.relative_to(output_root)),
                "image_path": "",
                "source": "gpt_fallback",
            }
        )
    return meta


def process_page_with_gpt_fallback(
    pdf_doc: pdfium.PdfDocument,
    page_no: int,
    output_root: Path,
    render_scale: float,
    gpt_api_key: str | None,
    gpt_model: str,
):
    if page_no <= 10:
        print(f"âš ï¸ Doclingì´ í˜ì´ì§€ {page_no}ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ˆê¸° í˜ì´ì§€ì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    if not gpt_api_key or gpt_api_key == GPT_API_KEY_PLACEHOLDER:
        print(
            f"âš ï¸ Doclingì´ í˜ì´ì§€ {page_no}ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆì§€ë§Œ GPT API Keyê°€ ì—†ì–´ ëŒ€ì²´ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤."
        )
        return

    print(f"ğŸ¤– [Fallback] GPT Visionìœ¼ë¡œ í˜ì´ì§€ {page_no} ë‚´ìš©ì„ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
    client = OpenAI(api_key=gpt_api_key)

    page_dir = output_root / f"page_{page_no:04d}"
    tables_dir = page_dir / "tables"
    figures_dir = page_dir / "figures"
    tables_dir.mkdir(parents=True, exist_ok=True)
    figures_dir.mkdir(parents=True, exist_ok=True)

    page_image = render_page_image(pdf_doc, page_no, render_scale)
    page_image_path = page_dir / "page.png"
    page_image.save(page_image_path)

    data_url = image_to_data_url(page_image)
    prompt = (
        "ë‹¹ì‹ ì€ ESG ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ì œê³µë˜ëŠ” í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”. "
        "ì¶œë ¥ í˜•ì‹: {\"summary\": str, \"key_points\": [str], \"tables\": ["
        "{\"title\": str, \"headers\": [str], \"rows\": [[str]]}], \"figures\": [str]}"
        "í‘œë¥¼ ì½ì„ ìˆ˜ ì—†ìœ¼ë©´ rowsëŠ” ë¹„ì›Œë‘ê³  ê°€ëŠ¥í•œ ì„¤ëª…ì„ key_pointsì— í¬í•¨í•˜ì„¸ìš”."
    )

    response = client.responses.create(
        model=gpt_model,
        input=[
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": "JSON ì´ì™¸ì˜ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.",
                    }
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": f"ESG ë³´ê³ ì„œ {page_no}í˜ì´ì§€ì…ë‹ˆë‹¤. ì§€ì‹œí•œ JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                    {"type": "input_image", "image_url": {"url": data_url}},
                ],
            },
        ],
        temperature=0.2,
        max_output_tokens=800,
    )

    raw_text = extract_response_text(response)
    try:
        extraction = json.loads(raw_text)
    except json.JSONDecodeError:
        extraction = {
            "summary": raw_text.strip(),
            "key_points": [],
            "tables": [],
            "figures": [],
        }

    page_md = page_dir / "page.md"
    markdown_text = build_markdown_from_gpt(extraction)
    if not markdown_text:
        markdown_text = "GPT ì¶”ì¶œ ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    page_md.write_text(markdown_text, encoding="utf-8")

    tables_meta = write_gpt_tables(extraction.get("tables", []), tables_dir, output_root)
    figures_meta: list[dict] = []
    for idx, caption in enumerate(extraction.get("figures", []) or [], start=1):
        if not caption:
            continue
        figures_meta.append(
            {
                "id": f"figure_{idx:03d}",
                "caption": caption,
                "image_path": str(page_image_path.relative_to(output_root)),
                "source": "gpt_fallback",
            }
        )

    page_width, page_height = get_pdf_page_size(pdf_doc, page_no)

    raw_json_path = page_dir / "gpt_raw.json"
    raw_json_path.write_text(json.dumps(extraction, ensure_ascii=False, indent=2), encoding="utf-8")

    page_payload = {
        "page_number": page_no,
        "markdown": markdown_text,
        "markdown_path": str(page_md.relative_to(output_root)),
        "page_image_path": str(page_image_path.relative_to(output_root)),
        "page_dimensions": {"width": page_width, "height": page_height},
        "tables": tables_meta,
        "figures": figures_meta,
        "needs_visual_review": True,
        "visual_density": 1.0,
        "summary_path": None,
        "gpt_fallback": True,
        "gpt_raw_path": str(raw_json_path.relative_to(output_root)),
    }

    page_json_path = page_dir / "page.json"
    page_json_path.write_text(json.dumps(page_payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"âœ… [Fallback] í˜ì´ì§€ {page_no}ë¥¼ GPTë¡œ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤ -> {page_json_path}")


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Docling ê²°ê³¼ë¥¼ í˜ì´ì§€ë³„ Markdown/í‘œ/ì´ë¯¸ì§€ë¡œ êµ¬ì¡°í™”í•œë‹¤.",
    )
    parser.add_argument(
        "--pdf",
        type=Path,
        default=None,
        help="ëŒ€ìƒ PDF ê²½ë¡œ. ìƒëµí•˜ë©´ data/inputì˜ ì²« ë²ˆì§¸ íŒŒì¼ì„ ì‚¬ìš©.",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=DEFAULT_OUTPUT_DIR,
        help="í˜ì´ì§€ë³„ ì‚°ì¶œë¬¼ì„ ì €ì¥í•  ë””ë ‰í„°ë¦¬.",
    )
    parser.add_argument(
        "--pages",
        type=str,
        default=None,
        help="1-based í˜ì´ì§€ ëª©ë¡/ë²”ìœ„(ì˜ˆ: 25,27-29).",
    )
    parser.add_argument(
        "--count",
        type=int,
        default=3,
        help="--pagesë¥¼ ìƒëµí•˜ë©´ ì•ìª½ì—ì„œ ëª‡ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í• ì§€ ì§€ì •.",
    )
    parser.add_argument(
        "--render-scale",
        type=float,
        default=2.0,
        help="PDF ì´ë¯¸ì§€ë¥¼ ë Œë”ë§í•  ë°°ìœ¨(ê¸°ë³¸ 2.0=ì•½ 144DPI).",
    )
    parser.add_argument(
        "--visual-threshold",
        type=float,
        default=0.35,
        help="í‘œ+ê·¸ë¦¼ ë©´ì  ë¹„ìœ¨ì´ ì´ ê°’ì„ ë„˜ìœ¼ë©´ ì´ë¯¸ì§€ í•´ì„ì´ í•„ìš”í•˜ë‹¤ê³  í‘œì‹œ.",
    )
    parser.add_argument(
        "--gpt-summary",
        action="store_true",
        help="í˜ì´ì§€ ë‹¨ìœ„ GPT ìš”ì•½ì„ ìƒì„±í•  ì¤€ë¹„ë¥¼ í•œë‹¤(ì‹¤ì œ í˜¸ì¶œì€ summarize_with_gptë¥¼ êµ¬í˜„í•´ì•¼ í•¨).",
    )
    parser.add_argument(
        "--gpt-api-key",
        type=str,
        default=None,
        help="GPT í˜¸ì¶œì— ì‚¬ìš©í•  API í‚¤. ìƒëµí•˜ë©´ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©.",
    )
    parser.add_argument(
        "--gpt-model",
        type=str,
        default="gpt-4o-mini",
        help="GPT í˜¸ì¶œì— ì‚¬ìš©í•  ëª¨ë¸ ID.",
    )
    parser.add_argument(
        "--report-name",
        type=str,
        default=None,
        help="pages_structured í•˜ìœ„ì— ìƒì„±í•  ë³´ê³ ì„œ í´ë” ì´ë¦„. ìƒëµí•˜ë©´ PDF íŒŒì¼ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ ìƒì„±.",
    )
    return parser


def main(argv: List[str] | None = None) -> int:
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    if args.pdf is None:
        pdf_path = infer_default_pdf()
        print(f"ê¸°ë³¸ PDF ì‚¬ìš©: {pdf_path}")
    else:
        pdf_path = args.pdf.expanduser().resolve()
        if not pdf_path.exists():
            parser.error(f"PDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")

    # Check for sanitized version automatically
    # Pattern 1: report.sanitized.pdf (Original tool default)
    # Pattern 2: report_sanitized.pdf (User modified convention)
    if "_sanitized" not in pdf_path.name and ".sanitized" not in pdf_path.name:
        candidates = [
            pdf_path.with_suffix(".sanitized.pdf"),
            pdf_path.with_stem(pdf_path.stem + "_sanitized").with_suffix(".pdf")
        ]
        
        for cand in candidates:
            if cand.exists():
                print(f"âš ï¸  [Auto-Switch] Sanitized PDF found: {cand.name}")
                print(f"    Switching input to use the sanitized version for better extraction.")
                pdf_path = cand
                break

    pdf_doc = pdfium.PdfDocument(str(pdf_path))
    total_pages = len(pdf_doc)

    if args.pages:
        pages = parse_page_selection(args.pages, total_pages)
    else:
        pages = pages_from_count(total_pages, args.count)

    target_pages = sorted(set(pages))

    def sanitize_report_name(raw: str) -> str:
        cleaned_chars = [ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in raw]
        sanitized = "".join(cleaned_chars).strip("_")
        return sanitized or "report"

    base_output = args.output_dir.resolve()
    default_base = DEFAULT_OUTPUT_DIR.resolve()
    report_name = args.report_name
    if not report_name and base_output == default_base:
        report_name = sanitize_report_name(pdf_path.stem)
    if report_name:
        output_root = base_output / report_name
    else:
        output_root = base_output
    output_root.mkdir(parents=True, exist_ok=True)

    gpt_api_key = (
        args.gpt_api_key
        or os.getenv("OPENAI_API_KEY")
        or GPT_API_KEY_PLACEHOLDER
    )

    converter = DocumentConverter()
    try:
        for start, end in chunk_consecutive(target_pages):
            result = converter.convert(pdf_path, page_range=(start, end))
            if result.status not in {ConversionStatus.SUCCESS, ConversionStatus.PARTIAL_SUCCESS}:
                errors = ", ".join(err.error_message for err in result.errors)
                raise RuntimeError(f"Docling ë³€í™˜ ì‹¤íŒ¨ ({start}-{end}): {result.status}. {errors}")
            if result.document is None:
                raise RuntimeError("Docling ë¬¸ì„œê°€ ë°˜í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # Analyze patterns for this batch
            current_batch_pages = [p for p in range(start, end + 1) if p in target_pages]
            clean_patterns = analyze_batch_patterns(result.document, current_batch_pages)

            for page_no in range(start, end + 1):
                if page_no not in target_pages:
                    continue
                has_docling_page = True
                try:
                    _ = result.document.pages[page_no]
                except Exception:
                    has_docling_page = False

                if has_docling_page:
                    process_page(
                        result.document,
                        pdf_doc,
                        page_no,
                        output_root,
                        args.render_scale,
                        gpt_api_key,
                        args.gpt_summary,
                        args.gpt_model,
                        args.visual_threshold,
                        clean_patterns,
                    )
                else:
                    process_page_with_gpt_fallback(
                        pdf_doc,
                        page_no,
                        output_root,
                        args.render_scale,
                        gpt_api_key,
                        args.gpt_model,
                    )
    finally:
        pdf_doc.close()

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
