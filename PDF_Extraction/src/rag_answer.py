"""
RAG ë‹µë³€ ìƒì„±ê¸° (Qwen2-VL ìµœì í™” ì ìš©)
ê¸°ëŠ¥: ë²¡í„° DB ê²€ìƒ‰ -> í˜ì´ì§€ ì´ë¯¸ì§€ ë¡œë“œ -> Qwen2-VL ëª¨ë¸ ë‹µë³€ ìƒì„±
"""

import argparse
import sys
import os
import time
from pathlib import Path
from typing import List, Dict, Optional

from dotenv import load_dotenv
import torch
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
from PIL import Image
from qwen_vl_utils import process_vision_info

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")

# ê²€ìƒ‰ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent))
try:
    from search_vector_db import search_vector_db, release_gpu
except ImportError:
    from search_vector_db import search_vector_db
    def release_gpu(): pass

DEFAULT_IMAGE_MAX_LONG_SIDE = 1024

def resize_image_if_needed(img: Image.Image, max_long_side: int) -> Image.Image:
    """ì´ë¯¸ì§€ ì¥ë³€ í¬ê¸° ì¡°ì ˆ (LANCZOS)"""
    w, h = img.size
    if max_long_side <= 0 or max(w, h) <= max_long_side:
        return img
    scale = max_long_side / max(w, h)
    return img.resize((int(w * scale), int(h * scale)), Image.Resampling.LANCZOS)

def get_page_image_path(metadata: Dict, page_no: Optional[int]) -> Optional[Path]:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í˜ì´ì§€ ì´ë¯¸ì§€ ê²½ë¡œ ì¶”ë¡ """
    if page_no is None:
        return None

    base_dir = Path("data/pages_structured")
    if not base_dir.exists():
        return None

    company = metadata.get('company_name') or metadata.get('company') or ''
    report_year = metadata.get('report_year') or metadata.get('year') or ''
    filename = metadata.get('filename') or ''
    
    # í›„ë³´ í´ë”ëª… ìƒì„±
    direct_report_dir = metadata.get('report_dir') or metadata.get('doc_dir')
    candidate_dirs = []
    if direct_report_dir: candidate_dirs.append(str(direct_report_dir).strip())
    
    if filename:
        stem = Path(filename).stem
        candidate_dirs.extend([stem, stem.replace(" ", "_"), stem.replace("-", "_")])
    
    if company and report_year:
        combos = [f"{report_year}_{company}_Report", f"{company}_{report_year}_Report"]
        for c in combos:
            candidate_dirs.extend([c, c.replace(" ", "_")])

    # í›„ë³´ ê²½ë¡œ íƒìƒ‰
    page_dir = f"page_{page_no:04d}"
    
    # 1. í›„ë³´ í´ë” ì§ì ‘ í™•ì¸
    for cand in candidate_dirs:
        path = base_dir / cand / page_dir / "page.png"
        if path.exists(): return path

    # 2. íšŒì‚¬/ì—°ë„ í¬í•¨ í´ë” ê²€ìƒ‰
    for folder in base_dir.iterdir():
        if folder.is_dir() and company.upper() in folder.name.upper() and str(report_year) in folder.name:
            path = folder / page_dir / "page.png"
            if path.exists(): return path

    return None

def main():
    parser = argparse.ArgumentParser(description="RAG ë‹µë³€ ìƒì„± (Qwen2-VL)")
    parser.add_argument("query", type=str, help="ì§ˆë¬¸ ë‚´ìš©")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2-VL-2B-Instruct", help="ì‚¬ìš© ëª¨ë¸ ID") 
    parser.add_argument("--top-k", type=int, default=3, help="ì°¸ì¡°í•  í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--max-tokens", type=int, default=512, help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--image-max-size", type=int, default=DEFAULT_IMAGE_MAX_LONG_SIDE, help="ì´ë¯¸ì§€ ìµœëŒ€ í¬ê¸°")
    parser.add_argument("--company", type=str, default=None, help="íšŒì‚¬ í•„í„°")
    parser.add_argument("--year", type=int, default=None, help="ì—°ë„ í•„í„°")
    
    args = parser.parse_args()
    
    # [1] ë²¡í„° DB ê²€ìƒ‰
    print(f"ğŸ” ê²€ìƒ‰: '{args.query}' (í•„í„°: {args.company or 'All'}, {args.year or 'All'})")
    t_start = time.time()
    results = search_vector_db(args.query, top_k=args.top_k, filter_company=args.company, filter_year=args.year)
    print(f"â±ï¸ ê²€ìƒ‰ ì†Œìš”: {time.time() - t_start:.4f}ì´ˆ")
    release_gpu()

    if not results:
        print("ê²°ê³¼ ì—†ìŒ.")
        return

    # [2] í˜ì´ì§€ ë°ì´í„° êµ¬ì„± (ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë³‘í•©)
    t_load_s = time.time()
    unique_pages = {}
    
    for res in results:
        meta = res.get('metadata', {})
        page_no = meta.get('page_no')
        if page_no is None: continue
        
        # ê³ ìœ  í‚¤ ìƒì„±
        key = f"{meta.get('company_name')}_{meta.get('report_year')}_{page_no}"
        
        if key not in unique_pages:
            img_path = get_page_image_path(meta, page_no)
            unique_pages[key] = {
                "image_path": img_path,
                "texts": [],
                "info": f"{meta.get('company_name')} {meta.get('report_year')} (p.{page_no})"
            }
        if res.get('content'):
            unique_pages[key]["texts"].append(res['content'])
            
    print(f"â±ï¸ ë°ì´í„° ë¡œë“œ: {time.time() - t_load_s:.4f}ì´ˆ")

    # [3] ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ: {args.model} (bfloat16)")
    t_model_s = time.time()
    try:
        import gc; gc.collect(); torch.cuda.empty_cache()
        
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            args.model,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        ).eval()
        
        processor = AutoProcessor.from_pretrained(
            args.model,
            min_pixels=256*28*28,
            max_pixels=1280*28*28,
            trust_remote_code=True
        )
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    print(f"â±ï¸ ëª¨ë¸ ë¡œë”©: {time.time() - t_model_s:.2f}ì´ˆ")

    # [4] í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_msg = "You are an ESG analyst. Cite evidence explicitly. Use provided context only."
    user_content = []
    
    # Top-K í˜ì´ì§€ë§Œ ì°¸ì¡°
    for _, data in list(unique_pages.items())[:args.top_k]:
        if data["image_path"]:
            img = resize_image_if_needed(Image.open(data["image_path"]), args.image_max_size)
            user_content.append({"type": "image", "image": img})
            user_content.append({"type": "text", "text": f"\n[Image: {data['info']}]\n"})
        
        text_dump = "\n".join(data["texts"])
        user_content.append({"type": "text", "text": f"\n[Text: {data['info']}]\n{text_dump}\n"})

    user_content.append({"type": "text", "text": f"{system_msg}\n\nQuestion: {args.query}"})
    messages = [{"role": "user", "content": user_content}]

    # [5] ë‹µë³€ ìƒì„±
    print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
    t_gen_s = time.time()
    
    text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    
    inputs = processor(
        text=[text_prompt],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=args.max_tokens, do_sample=False)
    
    # ì…ë ¥ í† í° ì œì™¸í•˜ê³  ë””ì½”ë”©
    generated_ids = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)]
    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    t_gen_e = time.time()
    gen_time = t_gen_e - t_gen_s
    num_tokens = output_ids.shape[1] - inputs.input_ids.shape[1]

    print("\n" + "="*40)
    print("ğŸ“ ë‹µë³€:")
    print("="*40)
    print(answer)
    print("="*40)
    print(f"â±ï¸ ìƒì„± ì‹œê°„: {gen_time:.2f}ì´ˆ ({num_tokens} í† í°, {num_tokens/gen_time:.1f} t/s)")
    print(f"â±ï¸ ì „ì²´ ì†Œìš”: {t_gen_e - t_start:.2f}ì´ˆ")

if __name__ == "__main__":
    main()
