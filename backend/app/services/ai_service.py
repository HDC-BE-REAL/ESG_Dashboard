import random
import sys
import time
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Callable, Generator, List, Optional, Tuple, Union
import openai

from ..config import settings

# [Ours] PDF ê²€ìƒ‰ ëª¨ë“ˆì„ ë™ì ìœ¼ë¡œ ë¡œë”©í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ ì¶”ê°€ (ê²½ë¡œ ë¬¸ì œ í•´ê²°)
# RAG ë¼ì´ë¸ŒëŸ¬ë¦¬(chromadb ë“±)ê°€ ì—†ì–´ë„ ì„œë²„ê°€ ì£½ì§€ ì•Šë„ë¡ ì˜ˆì™¸ ì²˜ë¦¬
def _load_pdf_search_helper() -> Optional[Callable]:
    """Attempt to import search_vector_db from PDF_Extraction/src."""
    try:
        from search_vector_db import search_vector_db as _search

        return _search
    except ModuleNotFoundError:
        base_dir = Path(__file__).resolve().parent.parent.parent.parent
        pdf_src = base_dir / "PDF_Extraction" / "src"
        if pdf_src.exists():
            if str(pdf_src) not in sys.path:
                sys.path.append(str(pdf_src))
            try:
                from search_vector_db import search_vector_db as _search

                return _search
            except Exception as exc:
                print(f"âš ï¸ [RAG] search_vector_db import failed: {exc}")
        return None
    except Exception as exc:
        print(f"âš ï¸ [RAG] search_vector_db import error: {exc}")
        return None


search_vector_db = _load_pdf_search_helper()

# RAG Libraries (Try import)
try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    HAS_RAG_LIBS = True
except ImportError:
    HAS_RAG_LIBS = False

class AIService:
    def __init__(self):
        if settings.OPENAI_API_KEY:
            openai.api_key = settings.OPENAI_API_KEY

        self.chroma_client = None
        self.collection = None  # legacy single collection
        self.chunk_collection = None
        self.page_collection = None
        self.embedding_model = None
        self.vector_db_path = self._resolve_vector_db_path()
        self.search_top_k = 5
        self.max_history_messages = 8
        self._initialization_lock = asyncio.Lock()
        self._is_initialized = False

    def _resolve_vector_db_path(self) -> Optional[Path]:
        if settings.VECTOR_DB_PATH:
            candidate = Path(settings.VECTOR_DB_PATH).expanduser()
            if candidate.exists():
                return candidate
            print(f"âš ï¸ [RAG] VECTOR_DB_PATH not found: {candidate}")

        base_dir = Path(__file__).resolve().parent.parent.parent.parent
        candidates = [base_dir / "PDF_Extraction" / "vector_db"]

        parent_dir = base_dir.parent    
        for repo_name in ("esg_pdf_extraction", "ESG_AIagent"):
            candidates.append(parent_dir / repo_name / "vector_db")

        for candidate in candidates:
            if candidate.exists():
                return candidate
        return None

    async def initialize(self):
        """Asynchronously initialize Vector DB and Embedding Model"""
        if self._is_initialized:
            return

        async with self._initialization_lock:
            if self._is_initialized:
                return

            print("â³ [RAG] Initializing AI Service (Loading models... this may take time)")
            try:
                loop = asyncio.get_running_loop()
                await loop.run_in_executor(None, self._init_vector_db_sync)
                self._is_initialized = True
                print("âœ… [RAG] AI Service initialized.")
            except Exception as e:
                print(f"âŒ [RAG Error] Failed to initialize Vector DB: {e}")
                self._is_initialized = True # Mark as initialized even if failed to avoid retry loop

    def _init_vector_db_sync(self):
        try:
            import chromadb
            from sentence_transformers import SentenceTransformer
        except ImportError:
            print("âš ï¸ [RAG] Required libraries (chromadb, sentence_transformers) not found. AI features limited.")
            return

        try:
            if settings.CHROMA_HOST:
                port = settings.CHROMA_PORT or 8000
                self.chroma_client = chromadb.HttpClient(host=settings.CHROMA_HOST, port=port)
                print(f"ğŸŒ [RAG] Connected to remote Chroma at {settings.CHROMA_HOST}:{port}")
            else:
                db_path = self.vector_db_path

                if not db_path:
                    print("âš ï¸ Vector DB ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PDF Extraction íŒŒì´í”„ë¼ì¸ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                    return

                self.chroma_client = chromadb.PersistentClient(path=str(db_path))
                print(f"ğŸ“ [RAG] Using local Chroma path: {db_path}")

            connected = False
            try:
                self.chunk_collection = self.chroma_client.get_collection("esg_chunks")
                connected = True
                print("âœ… [RAG] Connected to collection: esg_chunks")
            except Exception as e:
                print(f"âš ï¸ [RAG] Collection 'esg_chunks' not found: {e}")

            try:
                self.page_collection = self.chroma_client.get_collection("esg_pages")
                print("âœ… [RAG] Connected to collection: esg_pages")
            except Exception as e:
                print(f"âš ï¸ [RAG] Collection 'esg_pages' not found: {e}")

            if self.chunk_collection is None:
                try:
                    self.collection = self.chroma_client.get_collection("esg_documents")
                    connected = True
                    print("âœ… [RAG] Connected to collection: esg_documents (legacy)")
                except Exception as e:
                    print(f"âš ï¸ [RAG] Collection 'esg_documents' not found: {e}")

            if not connected:
                print("âš ï¸ [RAG] ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì»¬ë ‰ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return

            print("â³ [RAG] Loading embedding model BAAI/bge-m3...")
            self.embedding_model = SentenceTransformer("BAAI/bge-m3")
            print("âœ… [RAG] Embedding model loaded.")

        except Exception as e:
            print(f"âŒ [RAG Error] Failed to initialize Vector DB: {e}")

    @staticmethod
    def _content_to_text(content: Union[str, List, None]) -> str:
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            texts: List[str] = []
            for part in content:
                if isinstance(part, dict):
                    texts.append(part.get("text", ""))
                else:
                    text_value = getattr(part, "text", "")
                    texts.append(text_value)
            return "".join(texts)
        return str(content)

    async def generate_strategy(self, company_id: int, market: str, current_price: float):
        """
        íƒ„ì†Œ ë°°ì¶œê¶Œ ë§¤ìˆ˜ ì „ëµ ìƒì„± (Mock Data)
        """
        is_high_volatility = random.choice([True, False])
        
        tranches = []
        months = ["26.02", "26.03", "26.04", "26.05", "26.06", "26.07", "26.08", "26.09"]
        selected_months = random.sample(months, 3)
        selected_months.sort()

        if is_high_volatility:
            strategy_text = f"âš ï¸ [ê³ ë³€ë™ì„± ê°ì§€] {market} ì‹œì¥ì˜ ë³€ë™ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë¦¬ìŠ¤í¬ ë¶„ì‚°ì„ ìœ„í•´ 3íšŒì— ê±¸ì¹œ ë¶„í•  ë§¤ìˆ˜ ì „ëµì„ ì¶”ì²œí•©ë‹ˆë‹¤."
            percentages = [30, 40, 30]
        else:
            strategy_text = f"âœ… [ì•ˆì •ì  ì¶”ì„¸] {market} ì‹œì¥ì´ ì•ˆì •ì ì¸ íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì €ì  í™•ë³´ë¥¼ ìœ„í•œ ê³µê²©ì  ë§¤ìˆ˜ ì „ëµì´ ìœ íš¨í•©ë‹ˆë‹¤."
            percentages = [50, 30, 20]

        for i, month in enumerate(selected_months):
            forecast_price = current_price * (1 + random.uniform(-0.05, 0.05))
            tranches.append({
                "id": int(datetime.now().timestamp() * 1000) + i,
                "market": market,
                "price": round(forecast_price, 2) if market == "EU-ETS" else int(forecast_price),
                "month": month,
                "isFuture": True,
                "percentage": percentages[i]
            })

        return {
            "strategy_text": strategy_text,
            "tranches": tranches,
            "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M")
        }

    def _fast_path_response(self, message: str) -> Optional[str]:
        if "ì‹œë®¬ë ˆì´í„°" in message:
            return "ìƒë‹¨ì˜ 'ì‹œë®¬ë ˆì´í„°' íƒ­ì„ ëˆ„ë¥´ì‹œë©´ íƒ„ì†Œ ë¹„ìš© ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œë¥¼ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        return None


    def _is_last_year_query(self, message: str) -> bool:
        if not message:
            return False
        lower = message.lower()
        korean_keys = ["\uC791\uB144", "\uC9C0\uB09C\uD574", "\uC9C0\uB09C \uD574", "\uC791\uB144\uB3C4"]
        english_keys = ["last year", "previous year"]
        return any(k in message for k in korean_keys) or any(k in lower for k in english_keys)

    def _infer_latest_report_year(self, company_name: Optional[str], company_key: Optional[str]) -> Optional[int]:
        target = company_key or company_name
        if not target:
            return None

        if search_vector_db and self.vector_db_path:
            try:
                results = search_vector_db(
                    "\uC628\uC2E4\uAC00\uC2A4 \uBC30\uCD9C\uB7C9",
                    top_k=self.search_top_k,
                    semantic_top_k=max(self.search_top_k * 5, 40),
                    vector_db_path=str(self.vector_db_path),
                    filter_company=target,
                    verbose=False,
                )
                years = []
                for item in results or []:
                    meta = item.get("metadata", {}) if isinstance(item, dict) else {}
                    year = meta.get("report_year")
                    if year is not None:
                        try:
                            years.append(int(year))
                        except Exception:
                            continue
                if years:
                    return max(years)
            except Exception:
                pass

        collection = self.chunk_collection or self.page_collection or self.collection
        if not collection:
            return None
        years = []
        try:
            offset = 0
            limit = 500
            while True:
                batch = collection.get(include=["metadatas"], limit=limit, offset=offset)
                metas = batch.get("metadatas") or []
                if not metas:
                    break
                for meta in metas:
                    if self._metadata_matches(meta, company_name, company_key, None):
                        year = meta.get("report_year") if meta else None
                        if year is not None:
                            try:
                                years.append(int(year))
                            except Exception:
                                continue
                if len(metas) < limit:
                    break
                offset += len(metas)
        except Exception:
            return None
        return max(years) if years else None

    def _format_context_entry(self, document: str, metadata: dict) -> Tuple[str, Optional[str]]:
        company = metadata.get('company_name', 'Unknown')
        year = metadata.get('report_year', '????')
        page = metadata.get('page_no') or metadata.get('page_number') or metadata.get('page') or '?'
        source_type = metadata.get('source_type', 'page_text')
        title = metadata.get('table_title') or metadata.get('figure_title') or metadata.get('section_title')

        label_map = {
            'table': 'í‘œ',
            'figure': 'ê·¸ë¦¼',
            'page_text': 'ë³¸ë¬¸',
            'summary': 'ìš”ì•½'
        }
        label = label_map.get(source_type, 'ë³¸ë¬¸')

        header = f"[{company} {year} ë³´ê³ ì„œ p.{page} {label}]"
        if title:
            header += f" {title}"

        snippet = f"{header}: {str(document).strip()}\n\n"
        source_line = f"- {company} {year} Report (p.{page})"
        return snippet, source_line

    def _metadata_matches(
        self,
        metadata: Optional[dict],
        company_name: Optional[str] = None,
        company_key: Optional[str] = None,
        report_year: Optional[int] = None,
    ) -> bool:
        if not company_name and not company_key and report_year is None:
            return True
        if not metadata:
            return False
        meta_name = metadata.get('company_name') or metadata.get('company')
        meta_year = metadata.get('report_year')

        if report_year is not None and str(meta_year) != str(report_year):
            return False

        if not company_name and not company_key:
            return True

        if not meta_name:
            return False
        meta_norm = str(meta_name).strip().lower()

        if company_key:
            key_norm = company_key.strip().lower()
            if key_norm and (key_norm == meta_norm or key_norm in meta_norm or meta_norm in key_norm):
                return True

        if company_name:
            company_norm = company_name.strip().lower()
            if company_norm and (company_norm in meta_norm or meta_norm in company_norm):
                return True

        return False

    def _filter_doc_meta_pairs(
        self,
        pairs: List[Tuple[str, Optional[dict]]],
        company_name: Optional[str],
        company_key: Optional[str],
        report_year: Optional[int],
    ) -> List[Tuple[str, Optional[dict]]]:
        if not company_name and not company_key and report_year is None:
            return pairs
        filtered = [
            (doc, meta)
            for doc, meta in pairs
            if self._metadata_matches(meta, company_name, company_key, report_year)
        ]
        if filtered:
            return filtered
        # fallback to all if no match
        return pairs

    def _retrieve_context(
        self,
        message: str,
        company_name: Optional[str] = None,
        company_key: Optional[str] = None,
        report_year: Optional[int] = None,
    ) -> Tuple[str, List[str]]:
        if search_vector_db and self.vector_db_path:
            context, sources = self._retrieve_via_pdf_extraction(
                message, company_name, company_key, report_year
            )
            if context:
                return context, sources

        context_parts: List[str] = []
        source_info: List[str] = []

        if not self.embedding_model:
            return "", []

        collection = self.chunk_collection or self.page_collection or self.collection
        if not collection:
            # print("âš ï¸ [RAG] Vector collectionì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "", []

        try:
            print(f"ğŸ” [RAG] Searching for: {message}")
            query_vec = self.embedding_model.encode([message]).tolist()
            results = collection.query(
                query_embeddings=query_vec,
                n_results=5,
                include=["documents", "metadatas", "distances"]
            )

            if not results or not results.get('documents'):
                print("âš ï¸ [RAG] No results found.")
                return "", []

            docs = results['documents'][0]
            metas = results['metadatas'][0]
            doc_meta_pairs = self._filter_doc_meta_pairs(
                list(zip(docs, metas)), company_name, company_key, report_year
            )

            if (company_name or company_key) and not doc_meta_pairs:
                return "", []

            for doc, meta in doc_meta_pairs:
                snippet, source_line = self._format_context_entry(doc, meta or {})
                context_parts.append(snippet)
                if source_line not in source_info:
                    source_info.append(source_line)

            print(f"âœ… [RAG] Found {len(context_parts)} contexts.")

        except Exception as e:
            print(f"âŒ [RAG Search Error] {e}")

        return "".join(context_parts), source_info

    def _retrieve_via_pdf_extraction(
        self,
        message: str,
        company_name: Optional[str] = None,
        company_key: Optional[str] = None,
        report_year: Optional[int] = None,
    ) -> Tuple[str, List[str]]:
        try:
            if not search_vector_db:
                 return "", []

            results = search_vector_db(
                message,
                top_k=self.search_top_k,
                semantic_top_k=max(self.search_top_k * 5, 40),
                vector_db_path=str(self.vector_db_path) if self.vector_db_path else None,
                filter_company=company_key or company_name,
                filter_year=report_year,
                verbose=False,
            )
            if not results and (company_name or company_key or report_year):
                results = search_vector_db(  # type: ignore
                    message,
                    top_k=self.search_top_k,
                    semantic_top_k=max(self.search_top_k * 5, 40),
                    vector_db_path=str(self.vector_db_path) if self.vector_db_path else None,
                    verbose=False,
                )
        except Exception as exc:
            # print(f"âŒ [RAG Integration] search_vector_db failed: {exc}")
            return "", []

        if not results:
            return "", []

        filtered_results = [
            item
            for item in results
            if self._metadata_matches(item.get("metadata"), company_name, company_key, report_year)
        ]

        if company_name or company_key or report_year is not None:
            results_to_use = filtered_results or results
            if not filtered_results:
                print("âš ï¸ [RAG] ì„ íƒëœ í•„í„°ì— ë§ëŠ” ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ê²°ê³¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        else:
            results_to_use = filtered_results or results

        if not results_to_use:
            return "", []

        context_parts: List[str] = []
        source_info: List[str] = []
        for item in results_to_use:
            snippet, source_line = self._format_context_entry(item.get("content", ""), item.get("metadata", {}))
            context_parts.append(snippet)
            if source_line and source_line not in source_info:
                source_info.append(source_line)
        return "".join(context_parts), source_info

    def _build_messages(
        self,
        message: str,
        context: str,
        history: List[dict],
        company_name: Optional[str] = None,
        report_year: Optional[int] = None,
    ) -> List[dict]:
        system_prompt = (
            "You are an expert ESG consultant. "
            "Answer the user's question based on the provided Context if available. "
            "If the context provides specific data, cite the company and year. "
            "If the context is empty or irrelevant, answer using your general knowledge but mention that this is general advice. "
            "If the user does not specify a report year and multiple years exist, politely ask which ì—°ë„ ìë£Œê°€ í•„ìš”í•œì§€ instead of assuming. "
            "Speak in polite and professional Korean."
        )

        messages: List[dict] = [{"role": "system", "content": system_prompt}]
        trimmed_history = history[-self.max_history_messages :]
        for turn in trimmed_history:
            text = (turn.get("text") or "").strip()
            if not text:
                continue
            role = "assistant" if turn.get("role") == "assistant" else "user"
            messages.append({"role": role, "content": text})

        user_content = message.strip()
        if company_name or report_year:
            scope_note = company_name or "ì„ íƒëœ ê¸°ì—…"
            if report_year:
                scope_note += f" {report_year}ë…„"
            user_content += f"\n\n[ì„ íƒëœ ë²”ìœ„]\n{scope_note} ìë£Œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì„¸ìš”."

        if context:
            user_content += f"\n\n[Context]\n{context}\n\ní•´ë‹¹ ë¬¸ë§¥ì„ ìš°ì„  ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
        else:
            user_content += "\n\n[Context]\n(ê´€ë ¨ ë¬¸ë§¥ ì—†ìŒ: ì¼ë°˜ì ì¸ ì¡°ì–¸)"

        messages.append({"role": "user", "content": user_content})
        return messages

    async def get_chat_response(
        self,
        message: str,
        history: List[dict],
        company_name: Optional[str] = None,
        company_key: Optional[str] = None,
        report_year: Optional[int] = None,
    ):
        """
        RAG ê¸°ë°˜ AI ë‹µë³€ ìƒì„± (Vector DB + OpenAI)
        """
        # ì‘ë‹µ ìƒì„± ì‹œê°„ ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
        start_time = time.perf_counter()
        # ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¹„ë™ê¸° ì´ˆê¸°í™” ìˆ˜í–‰
        if not self._is_initialized:
            await self.initialize()

        fast_response = self._fast_path_response(message)
        if fast_response:
            return fast_response

        is_last_year_query = self._is_last_year_query(message)
        if report_year is None and is_last_year_query and (company_name or company_key):
            inferred_year = self._infer_latest_report_year(company_name, company_key)
            if inferred_year:
                report_year = inferred_year
                message = f"{message}\n\n[Interpretation] 'last year' means data year (report_year-1) from the latest report. Provide {report_year - 1} values."

        context_start = time.perf_counter()
        context, source_info = self._retrieve_context(
            message, company_name, company_key, report_year
        )
        print(
            f"â±ï¸ [Perf] Context retrieval took {time.perf_counter() - context_start:.2f}s"
        )

        if (company_name or company_key or report_year) and not context:
            target = company_name or company_key or (
                f"{report_year}ë…„ ë³´ê³ ì„œ" if report_year else "ì„ íƒëœ ë²”ìœ„"
            )
            return (
                f"âš ï¸ {target} ê´€ë ¨ ESG ë³´ê³ ì„œë¥¼ Vector DBì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                "PDF ì—…ë¡œë“œ ë˜ëŠ” ë²¡í„° DB ë™ê¸°í™”ë¥¼ ë¨¼ì € ì§„í–‰í•´ ì£¼ì„¸ìš”."
            )

        if not settings.OPENAI_API_KEY:
            return "âš ï¸ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

        messages = self._build_messages(message, context, history, company_name, report_year)

        try:
            llm_start = time.perf_counter()
            client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            response = await client.chat.completions.create(
                model="gpt-4o",  # or gpt-3.5-turbo
                messages=messages,
                temperature=0.7,
                max_tokens=600
            )
            print(f"â±ï¸ [Perf] LLM completion took {time.perf_counter() - llm_start:.2f}s")
            print(f"â±ï¸ [Perf] Total latency {time.perf_counter() - start_time:.2f}s")

            answer = self._content_to_text(response.choices[0].message.content)

            if source_info:
                answer += "\n\nğŸ“š **ì°¸ê³  ë¬¸í—Œ:**\n" + "\n".join(source_info)

            return answer

        except Exception as e:
            print(f"LLM Error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (OpenAI API ì—°ê²° ì‹¤íŒ¨)"

    # [Integrated] ë¹„ë™ê¸° ì œë„ˆë ˆì´í„°(Theirs) + í•„í„°ë§ íŒŒë¼ë¯¸í„°(Ours)
    async def stream_chat_response(
        self,
        message: str,
        history: List[dict],
        company_name: Optional[str] = None,
        company_key: Optional[str] = None,
        report_year: Optional[int] = None,
    ) -> Generator[str, None, None]:
        if not self._is_initialized:
            await self.initialize()
        fast_response = self._fast_path_response(message)
        if fast_response:
            yield fast_response
            return

        is_last_year_query = self._is_last_year_query(message)
        if report_year is None and is_last_year_query and (company_name or company_key):
            inferred_year = self._infer_latest_report_year(company_name, company_key)
            if inferred_year:
                report_year = inferred_year

        context_start = time.perf_counter()
        context, source_info = self._retrieve_context(
            message, company_name, company_key, report_year
        )
        print(
            f"â±ï¸ [Perf] Context retrieval took {time.perf_counter() - context_start:.2f}s"
        )

        if (company_name or company_key or report_year) and not context:
            target = company_name or company_key or (
                f"{report_year}ë…„ ë³´ê³ ì„œ" if report_year else "ì„ íƒëœ ë²”ìœ„"
            )
            yield (
                f"âš ï¸ {target} ê´€ë ¨ ESG ë³´ê³ ì„œë¥¼ Vector DBì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                "PDF ì—…ë¡œë“œ ë˜ëŠ” ë²¡í„° DB ë™ê¸°í™”ë¥¼ ë¨¼ì € ì§„í–‰í•´ ì£¼ì„¸ìš”."
            )
            return

        if not settings.OPENAI_API_KEY:
            yield "âš ï¸ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            return

        messages = self._build_messages(message, context, history, company_name, report_year)

        try:
            client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            stream = await client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.7,
                max_tokens=600,
                stream=True
            )

            async for chunk in stream:
                if not chunk.choices:
                    continue
                delta = chunk.choices[0].delta
                if not delta:
                    continue
                content = self._content_to_text(getattr(delta, "content", None))
                if content:
                    yield content

            if source_info:
                yield "\n\nğŸ“š **ì°¸ê³  ë¬¸í—Œ:**\n" + "\n".join(source_info)

        except Exception as e:
            print(f"LLM Stream Error: {e}")
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (OpenAI API ì—°ê²° ì‹¤íŒ¨)"

    async def text_to_sql(self, question: str, db_schema: str = None):
        """
        ìì—°ì–´ë¥¼ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜ (Mock)
        """
        return f"SELECT * FROM documents WHERE content LIKE '%{question}%' LIMIT 5;"

    async def generate_compare_insight(self, my_company: str, intensity_type: str, my_intensity: float, median_intensity: float, top10_intensity: float, best_company: str, is_better_than_median: bool) -> str:
        """
        ê²½ìŸì‚¬ ë¹„êµ íƒ­ì—ì„œ ì „ëµì  ì¸ì‚¬ì´íŠ¸ ë¬¸êµ¬ë¥¼ ìƒì„± (Few-shot prompting ë°©ì‹ + Fallback)
        """
        diff_to_top10 = max(0, my_intensity - top10_intensity)
        pct_to_top10 = (diff_to_top10 / my_intensity * 100) if my_intensity > 0 else 0
        intensity_label = "íƒ„ì†Œ ì§‘ì•½ë„" if intensity_type == 'revenue' else "ì—ë„ˆì§€ ì§‘ì•½ë„"
        
        fallback_text = "í˜„ì¬ ì¼ë¶€ ì˜¤ë¥˜ê°€ ìˆì–´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶œë ¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        if not settings.OPENAI_API_KEY:
            return fallback_text

        prompt = f"""ë‹¹ì‹ ì€ ë‚ ì¹´ë¡œìš´ í†µì°°ë ¥ì„ ì§€ë‹Œ ESG ì „ëµ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—… ë¡œê³  ì˜†ì— ë„ìš¸ ì§§ê³  ê°•ë ¥í•œ 'ì „ëµì  ì¸ì‚¬ì´íŠ¸' 2ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. HTML íƒœê·¸(<strong class="text-white">, <span class="text-[#10b77f] font-bold">, <span class="text-white underline decoration-[#10b77f]/50 decoration-2 underline-offset-4"> ë“±)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ í•µì‹¬ ë¬¸êµ¬ë¥¼ ê°•ì¡°í•´ì•¼ í•©ë‹ˆë‹¤. ë§ˆí¬ë‹¤ìš´ì˜ **êµµê²Œ**ëŠ” ì“°ì§€ ë§ê³  HTMLë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

[ë°ì´í„°]
- ë¶„ì„ ëŒ€ìƒ ê¸°ì—…: ìš°ë¦¬ ê¸°ì—…
- ì§€í‘œ ì¢…ë¥˜: {intensity_label} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
- ìš°ë¦¬ ê¸°ì—…ì˜ ì§€í‘œ ê°’: {my_intensity:.2f}
- ì—…ê³„ í‰ê· (Median): {median_intensity:.2f}
- ìƒìœ„ 10% ì»·ì˜¤í”„: {top10_intensity:.2f}
- ìƒìœ„ 10% ì§„ì…ì„ ìœ„í•œ í•„ìš” ê°ì¶•ë¥ : ì•½ {pct_to_top10:.1f}%
- 1ìœ„ ê¸°ì—…(ì„ ë‘): {best_company}
- ìš°ë¦¬ ê¸°ì—…ì´ í‰ê· ë³´ë‹¤ ìš°ìˆ˜í•œê°€? {'ì˜ˆ' if is_better_than_median else 'ì•„ë‹ˆì˜¤'}

[ì˜ˆì‹œ 1 - í‰ê·  ìƒíšŒ ì‹œ]
<strong class="text-white">ìš°ë¦¬ ê¸°ì—…</strong>ì€ í˜„ì¬ ì—…ê³„ í‰ê· (Median)ì„ ìƒíšŒí•˜ëŠ” íš¨ìœ¨ì„±ì„ ë³´ì´ê³  ìˆìœ¼ë‚˜, ìƒìœ„ 10% ì§„ì…ì„ ìœ„í•´ì„œëŠ” {intensity_label}ì˜ <span class="text-[#10b77f] font-bold">15.0% ì¶”ê°€ ê°ì¶•</span>ì´ í•„ìš”í•©ë‹ˆë‹¤. ì„ ë‘ ê¸°ì—…ì˜ ë…ë³´ì  ê²½ìŸë ¥ì€ ê³µê¸‰ë§ íƒ„ì†Œ ê´€ë¦¬ì—ì„œì˜ <span class="text-white underline decoration-[#10b77f]/50 decoration-2 underline-offset-4">Scope 3 ê°ì¶• ê¸°ìˆ ë ¥</span>ì—ì„œ ê¸°ì¸í•©ë‹ˆë‹¤.

[ì˜ˆì‹œ 2 - í‰ê·  í•˜íšŒ ì‹œ]
<strong class="text-white">ìš°ë¦¬ ê¸°ì—…</strong>ì€ í˜„ì¬ ì—…ê³„ í‰ê· ì— ë¯¸ì¹˜ì§€ ëª»í•˜ê³  ìˆìœ¼ë©°, ê¸€ë¡œë²Œ íƒ‘í‹°ì–´ ë„ì•½ì„ ìœ„í•´ì„œëŠ” {intensity_label}ì˜ <span class="text-[#10b77f] font-bold">32.5% ëŒ€í­ ê°ì¶•</span>ì´ ì‹œê¸‰í•©ë‹ˆë‹¤. ì„ ë‘ ê¸°ì—…ì˜ ì›ë™ë ¥ì€ í™”ì„ì—°ë£Œ ë¹„ì¤‘ì„ ëŒ€í­ ì¤„ì¸ <span class="text-white underline decoration-[#10b77f]/50 decoration-2 underline-offset-4">ì¬ìƒì—ë„ˆì§€ 100% ì „í™˜</span>ì…ë‹ˆë‹¤.

[ì˜ˆì‹œ 3 - ìƒìœ„ê¶Œ ë‹¬ì„± ì‹œ]
<strong class="text-white">ìš°ë¦¬ ê¸°ì—…</strong>ì€ ì´ë¯¸ ì—…ê³„ ìƒìœ„ 10%ì— ì§„ì…í•˜ì—¬ ì••ë„ì ì¸ í¬ì§€ì…˜ì„ ì í•˜ê³  ìˆìŠµë‹ˆë‹¤. 1ìœ„ì™€ì˜ ê²©ì°¨ë¥¼ ì—†ì• ë ¤ë©´ <span class="text-[#10b77f] font-bold">í˜ì‹  ê³µì • ë„ì…</span>ì„ í†µí•œ í•œê³„ ëŒíŒŒê°€ í•„ìš”í•˜ë©°, ì„ ë‘ ê¸°ì—…({best_company})ì´ ì„ ì í•œ <span class="text-white underline decoration-[#10b77f]/50 decoration-2 underline-offset-4">íƒ„ì†Œí¬ì§‘(CCUS) ê¸°ìˆ </span> ìƒìš©í™”ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•´ì•¼ í•©ë‹ˆë‹¤.

"""

        try:
            client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            response = await client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=300
            )
            content = self._content_to_text(response.choices[0].message.content)
            return content.strip()
        except Exception as e:
            print(f"Compare Insight GPT Error: {e}")
            return fallback_text

ai_service = AIService()
