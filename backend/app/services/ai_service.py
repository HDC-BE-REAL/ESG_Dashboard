import random
from datetime import datetime
import os
from pathlib import Path
from typing import List, Optional
import openai
from ..config import settings

# RAG Libraries (Try import)
try:
    import chromadb
    from sentence_transformers import SentenceTransformer
    HAS_RAG_LIBS = True
except ImportError:
    HAS_RAG_LIBS = False

class AIService:
    def __init__(self):
        if settings.OPENAI_API_KEY:
            openai.api_key = settings.OPENAI_API_KEY
        
        self.chroma_client = None
        self.collection = None
        self.embedding_model = None
        
        if HAS_RAG_LIBS:
            self._init_vector_db()

    def _init_vector_db(self):
        try:
            # Absolute path to Vector DB
            # Assuming standard project structure: /home/dmin/ESG_Wep/PDF_Extraction/vector_db
            base_dir = Path(__file__).resolve().parent.parent.parent.parent
            db_path = base_dir / "PDF_Extraction" / "vector_db"
            
            if not db_path.exists():
                print(f"âš ï¸ Vector DB path not found: {db_path}")
                return

            self.chroma_client = chromadb.PersistentClient(path=str(db_path))
            
            # Try to get the collection (using 'esg_documents' as found in inspection)
            try:
                self.collection = self.chroma_client.get_collection("esg_documents")
                print("âœ… [RAG] Connected to collection: esg_documents")
            except Exception as e:
                print(f"âš ï¸ [RAG] Collection 'esg_documents' not found: {e}")
                # Fallback to other names if needed, but for now stick to what we found
                return

            # Initialize Embedding Model
            # This might take a moment on first load
            print("â³ [RAG] Loading embedding model BAAI/bge-m3...")
            self.embedding_model = SentenceTransformer("BAAI/bge-m3")
            print("âœ… [RAG] Embedding model loaded.")

        except Exception as e:
            print(f"âŒ [RAG Error] Failed to initialize Vector DB: {e}")

    async def generate_strategy(self, company_id: int, market: str, current_price: float):
        """
        íƒ„ì†Œ ë°°ì¶œê¶Œ ë§¤ìˆ˜ ì „ëµ ìƒì„± (Mock Data)
        """
        is_high_volatility = random.choice([True, False])
        
        tranches = []
        months = ["26.02", "26.03", "26.04", "26.05", "26.06", "26.07", "26.08", "26.09"]
        selected_months = random.sample(months, 3)
        selected_months.sort()

        if is_high_volatility:
            strategy_text = f"âš ï¸ [ê³ ë³€ë™ì„± ê°ì§€] {market} ì‹œì¥ì˜ ë³€ë™ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë¦¬ìŠ¤í¬ ë¶„ì‚°ì„ ìœ„í•´ 3íšŒì— ê±¸ì¹œ ë¶„í•  ë§¤ìˆ˜ ì „ëµì„ ì¶”ì²œí•©ë‹ˆë‹¤."
            percentages = [30, 40, 30]
        else:
            strategy_text = f"âœ… [ì•ˆì •ì  ì¶”ì„¸] {market} ì‹œì¥ì´ ì•ˆì •ì ì¸ íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì €ì  í™•ë³´ë¥¼ ìœ„í•œ ê³µê²©ì  ë§¤ìˆ˜ ì „ëµì´ ìœ íš¨í•©ë‹ˆë‹¤."
            percentages = [50, 30, 20]

        for i, month in enumerate(selected_months):
            forecast_price = current_price * (1 + random.uniform(-0.05, 0.05))
            tranches.append({
                "id": int(datetime.now().timestamp() * 1000) + i,
                "market": market,
                "price": round(forecast_price, 2) if market == "EU-ETS" else int(forecast_price),
                "month": month,
                "isFuture": True,
                "percentage": percentages[i]
            })

        return {
            "strategy_text": strategy_text,
            "tranches": tranches,
            "analysis_date": datetime.now().strftime("%Y-%m-%d %H:%M")
        }

    async def get_chat_response(self, message: str):
        """
        RAG ê¸°ë°˜ AI ë‹µë³€ ìƒì„± (Vector DB + OpenAI)
        """
        # 1. íŠ¹ì • í‚¤ì›Œë“œ ì²˜ë¦¬ (Fast Path)
        if "ì‹œë®¬ë ˆì´í„°" in message:
            return "ìƒë‹¨ì˜ 'ì‹œë®¬ë ˆì´í„°' íƒ­ì„ ëˆ„ë¥´ì‹œë©´ íƒ„ì†Œ ë¹„ìš© ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œë¥¼ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        # 2. RAG ê²€ìƒ‰
        context = ""
        source_info = []
        
        if self.collection and self.embedding_model:
            try:
                print(f"ğŸ” [RAG] Searching for: {message}")
                # Embed query
                query_vec = self.embedding_model.encode([message]).tolist()
                
                # Query DB
                results = self.collection.query(
                    query_embeddings=query_vec,
                    n_results=3,
                    include=["documents", "metadatas", "distances"]
                )
                
                if results and results['documents']:
                    docs = results['documents'][0]
                    metas = results['metadatas'][0]
                    
                    for doc, meta in zip(docs, metas):
                        company = meta.get('company_name', 'Unknown')
                        year = meta.get('report_year', '????')
                        page = meta.get('page_no', '?')
                        
                        source_line = f"- {company} {year} Report (p.{page})"
                        if source_line not in source_info:
                            source_info.append(source_line)
                            
                        texts_part = f"[{company} {year} Report p.{page}]: {doc}"
                        context += texts_part + "\n\n"
                    
                    print(f"âœ… [RAG] Found {len(docs)} contexts.")
                else:
                    print("âš ï¸ [RAG] No results found.")
            except Exception as e:
                print(f"âŒ [RAG Search Error] {e}")

        # 3. LLM í˜¸ì¶œ
        if not settings.OPENAI_API_KEY:
            return "âš ï¸ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."

        try:
            system_prompt = (
                "You are an expert ESG consultant. "
                "Answer the user's question based on the provided Context if available. "
                "If the context provides specific data, cite the company and year. "
                "If the context is empty or irrelevant, answer using your general knowledge but mention that this is general advice. "
                "Speak in polite and professional Korean."
            )

            user_prompt = f"Question: {message}\n\n"
            if context:
                user_prompt += f"Context:\n{context}\n\n"
                user_prompt += "Based on the context above, answer the question."
            else:
                user_prompt += "Answer based on your general knowledge."

            client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
            response = client.chat.completions.create(
                model="gpt-4o",  # or gpt-3.5-turbo
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                max_tokens=600
            )
            
            answer = response.choices[0].message.content
            
            # ì¶œì²˜ ì¶”ê°€
            if source_info:
                answer += "\n\nğŸ“š **ì°¸ê³  ë¬¸í—Œ:**\n" + "\n".join(source_info)

            return answer

        except Exception as e:
            print(f"LLM Error: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (OpenAI API ì—°ê²° ì‹¤íŒ¨)"

    async def text_to_sql(self, question: str, db_schema: str = None):
        """
        ìì—°ì–´ë¥¼ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜ (Mock)
        """
        return f"SELECT * FROM documents WHERE content LIKE '%{question}%' LIMIT 5;"

ai_service = AIService()
